{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install thop","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:49:10.955149Z","iopub.execute_input":"2023-03-15T12:49:10.955854Z","iopub.status.idle":"2023-03-15T12:49:22.007996Z","shell.execute_reply.started":"2023-03-15T12:49:10.955807Z","shell.execute_reply":"2023-03-15T12:49:22.006848Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from thop) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->thop) (4.4.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport time\nfrom tqdm import tqdm\nimport sys\nimport torch.nn.functional as F\nfrom torch.nn.functional import cross_entropy\nimport os\nfrom thop import profile\nimport pandas as pd\nimport glob\nimport math\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# 配置运行设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 设置参数\nbatch_size = 32\nlearning_rate = 0.00003\nnum_epoch = 70\nmodel_name = 'capsnet'\npc_out_dim = 16\nout_dim = 16\nn_classes = 2","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:51:47.401369Z","iopub.execute_input":"2023-03-15T12:51:47.402449Z","iopub.status.idle":"2023-03-15T12:51:47.410724Z","shell.execute_reply.started":"2023-03-15T12:51:47.402398Z","shell.execute_reply":"2023-03-15T12:51:47.409477Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# 数据处理\nnormalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\ntransform = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor(),\n    normalize\n])\n\n# 读取图像数据\ntrain_dataset = ImageFolder('/kaggle/input/breakhis-total-dataset/train/', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = ImageFolder('/kaggle/input/breakhis-total-dataset/test/', transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\nprint('{0} for train. {1} for val'.format(len(train_dataset), len(test_dataset)))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:49:40.495010Z","iopub.execute_input":"2023-03-15T12:49:40.495955Z","iopub.status.idle":"2023-03-15T12:49:50.430492Z","shell.execute_reply.started":"2023-03-15T12:49:40.495915Z","shell.execute_reply":"2023-03-15T12:49:50.429373Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"8486 for train. 2122 for val\n","output_type":"stream"}]},{"cell_type":"code","source":"def squash(x):\n    '''\n    squash函数，保证向量长度范围在(0, 1)\n    :param x:\n        x: (B, 10, 16)\n    :return:\n        squashed x (B, 10, 16)\n    '''\n    L = torch.norm(x, dim=2, keepdim=True)  # (B, 10, 1)\n    L_square = L ** 2  # (B, 10, 1)\n    c = L_square / ((1 + L_square) * L)\n\n    s = c * x  # (B, 10, 16)\n    s[s == np.nan] = 0\n    return s\n\n\ndef dynamic_routing(x, iterations=3):\n    \"\"\"\n    动态路由\n    :param x:\n        x: (B, classes, 48*7*7, out_channels_dim, 1)\n    :param iterations:\n    :return:\n        v: next layer output (B, classes, out_channels_dim)\n    \"\"\"\n\n    N = x.shape[2]  # 输入的向量个数，此处输入32个胶囊，每个胶囊提供7*7个向量\n    N1 = x.shape[1]  # 输出类别个数\n\n\n    # 为每个向向量配置一个权重(初始为0)，并依据此权重累加向量(以类别为单位)，通过squash显示累加向量长度\n    # 计算每个初始的每个向量与累加向量的内积，来更新该向量的权重\n    b = torch.zeros(1, N1, N, 1, 1).to(x.device)  # (B, classes, 48*11*11, 1, 1)\n    for _ in range(iterations):\n        c = F.softmax(b, dim=2)  # (1, classes, 32*7*7, 1, 1)\n        s = torch.sum(x.matmul(c), dim=2).squeeze(-1)  # (B, classes, out_channels_dim)\n        v = squash(s)  # (B, classes, out_channels_dim)\n        b = b + v[:, :, None, None, :].matmul(x)\n        # (B, classes, 1, 1, out_channels_dim) .* (B, classes, 32*11*11, out_channels_dim, 1)\n        # = (B, classes, 32*11*11, 1, 1)\n    return v\n\n\nclass PrimaryCapsuleLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.primary_capsule_layer = nn.ModuleList([nn.Conv2d(512, pc_out_dim, 3, stride=1, padding=1) for _ in range(32)])\n        self.out_dim = pc_out_dim\n\n    def forward(self, x):\n        \"\"\"\n        Produce primary capsules\n        :param x:\n            x : features with # [B, 256, 14, 14]\n        :return:\n            vectors (B, 64*7*7, 128)\n        \"\"\"\n        capsules = [conv(x) for conv in self.primary_capsule_layer]  # 128 * [B, 64, 1, 1]\n        capsules_reshaped = [c.reshape(-1, self.out_dim, 7 * 7) for c in capsules]\n        s = torch.cat(capsules_reshaped, dim=-1).permute(0, 2, 1)  # (B, 128*1*1, 64)\n        return s\n\n\nclass CapsLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(1, n_classes, 32*7*7, out_dim, pc_out_dim))\n        self.classes = n_classes\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        \"\"\"\n        predict and routing\n        :param x:\n            x : input vectors, # (B, 64*14*14, 8)\n        :return:\n            class capsules, (B, 2, 64)\n        \"\"\"\n        x = x[:, None, ..., None]  # (B, 1, 32*7*7, 16, 1)\n        # W : (1, classes, 32*11*11, out_channel_dim, 16)\n        u_hat = self.W.matmul(x)  # (B, classes, 32*11*11, out_channels_dim, 1)\n        assert u_hat.shape[1:] == (self.classes, 32*7*7, self.out_dim, 1), 'dim is wrong'\n        class_capsules = dynamic_routing(u_hat, iterations=3)\n        return class_capsules\n\n\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.channels = channels\n        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n\n        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        y = self.relu(self.conv1(x))\n        y = self.conv2(y)\n        return self.relu(x + y)\n\n\nclass InceptionBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(InceptionBlock, self).__init__()\n        self.conv_s1 = torch.nn.Conv2d(channels, channels, kernel_size=1)\n        self.conv_s3 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.conv_s5 = torch.nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n        self.conv_s7 = torch.nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        y = torch.cat((self.conv_s1(x), self.conv_s3(x), self.conv_s5(x), self.conv_s7(x)), dim=1)\n        y = self.relu(y)\n        return y\n\nclass CNNdecoder(nn.Module):\n    '''\n    对输入的向量进行解码使其变为原图片\n\n\n    input_shape: [batch_size, vector_length]\n    output_shape: [batch_size, 3, 224, 224]\n\n    '''\n    def __init__(self, out_channels_dim):\n        super(CNNdecoder, self).__init__()\n        self.W = nn.Parameter(torch.randn(1, 14*14, 2))  # (1, 2, 32*12*12, out_channels_dim, 8)\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.conv_layer1 = nn.Conv2d(out_channels_dim, 96, kernel_size=3, padding=1)\n        self.conv_layer2 = nn.Conv2d(96, 48, kernel_size=3, padding=1)\n        self.conv_layer3 = nn.Conv2d(48, 24, kernel_size=3, padding=1)\n        self.conv_layer4 = nn.Conv2d(24, 12, kernel_size=3, padding=1)\n        self.conv_layer5 = nn.Conv2d(12, 6, kernel_size=3, padding=1)\n        self.conv_layer6 = nn.Conv2d(6, 3, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        B = x.shape[0]\n        v_length = x.shape[2]\n#         x = x.reshape(B, 1, v_length)\n        try:\n            y = self.W.matmul(x)\n        except:\n            print('x shape:',x.shape)\n            print('W shape:',self.W.shape)                    # [B, 14*14, v_length]\n        y = y.permute(0, 2, 1).reshape((B, v_length, 14, 14))    # [B, v_length, 14, 14]\n        y = self.relu(self.conv_layer1(self.upsample(y)))  # [B, 96, 28, 28]\n        y = self.relu(self.conv_layer2(self.upsample(y)))  # [B, 48, 56, 56]\n        y = self.relu(self.conv_layer3(y))   # [B, 24, 56, 56]\n        y = self.relu(self.conv_layer4(self.upsample(y)))  # [B, 12, 112, 112]\n        y = self.relu(self.conv_layer5(self.upsample(y)))  # [B, 6, 224, 224]\n        y = self.relu(self.conv_layer6(y)) \n        return y\n\n# 解码器\nclass MLPDecoder(nn.Module):\n    \"\"\"\n    Decoder the input predicted vectors to origin images\n\n    Usages:\n        decoder = MLPDecoder([512, 1024], 16, 3, (28, 28)\n        reconstructed_x = decoder(selected_capsules)\n    \"\"\"\n    def __init__(self, hidden, in_channels, out_channels, out_shape):\n        super().__init__()\n        self.out_shape = out_shape\n        h, w = out_shape\n        self.out_channels = out_channels\n        self.out_size = h * w\n        self.mlp = nn.ModuleList()\n        for _ in range(out_channels):\n            mlp_part = nn.Sequential()\n            for i, (_in, _out) in enumerate(zip([in_channels] + hidden[:-1], hidden)):\n                mlp_part.add_module('Linear{0}'.format(i), nn.Linear(_in, _out))\n                mlp_part.add_module('Relu{0}'.format(i), nn.ReLU())\n            mlp_part.add_module('Linear{0}'.format(i+1), nn.Linear(hidden[-1], self.out_size))\n            mlp_part.add_module('Sigmoid{0}'.format(i+1), nn.Sigmoid())\n            self.mlp.append(mlp_part)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B,16)\n\n        Return:\n            reconstructed images with (B,1,28,28)\n        \"\"\"\n        B = x.shape[0]\n        output = torch.zeros((B, self.out_channels, *self.out_shape)).to(x.device)\n        for i in range(self.out_channels):\n            k = self.mlp[i]\n            out = self.mlp[i](x)\n            output[:, i, :, :] = out.reshape(B, *self.out_shape)\n        return output\n\n\n\nclass CapsNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        '''\n        input_shape: [B, 3, 224, 224]\n\n        '''\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv_layer1 = nn.Conv2d(3, 16, 5, stride=1, padding=2)\n        self.conv_layer2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv_layer3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv_layer4 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv_layer5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n        self.conv_layer6 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n        self.inception = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n        self.conv_layer_res = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.primary_layer = PrimaryCapsuleLayer()\n        self.caps_layer = CapsLayer()\n\n    def forward(self, x):\n        \"\"\"\n        args:\n            x: Input img, (B, 3, 224, 224)\n        return:\n            the calss capsules, ench capsule is a 16 dimension vector\n\n        \"\"\"\n        x = self.relu(self.conv_layer1(x))  # [B, 16, 224, 224]\n        x = self.maxpool(x)# [B, 16, 112, 112]\n        x = self.relu(self.conv_layer2(x))\n        y = self.maxpool(x)# [B, 32, 56, 56]\n        \n        \n        x = self.relu(self.conv_layer3(y))\n        x = self.maxpool(x) # [B, 64, 28, 28]\n        x = self.conv_layer4(x) # [B, 128, 28, 28]\n        \n        y = self.inception(y) # [B, 128, 56, 56]\n        y = self.maxpool(self.conv_layer_res(y)) # [B, 128, 28, 28]\n        \n        x = self.relu(x + y)\n        x = self.maxpool(x) # [B, 128, 14, 14]\n        x = self.relu(self.conv_layer5(x)) # [B, 256, 14, 14]\n        x = self.maxpool(x)\n        x = self.relu(self.conv_layer6(x)) # [B, 512, 7, 7]\n        x = self.primary_layer(x)  # [B, 6*6*48, 8]\n        x = self.caps_layer(x)  # (2, 48*6*6, 48, 1)\n        return x\n\n\ndef margin_loss(y_hat, y, wrong_weight=0.5):\n    '''\n    args:\n        y : ground truth labels (B)\n        y_hat : class capsules with (B, 10, 16)\n\n    return:\n        the margin loss\n    '''\n    _lambda = 0.5\n    m_plus = 0.9\n    m_minus = 0.1\n\n    y_norm = y_hat.norm(dim=-1)\n    y = y.type(torch.int64)\n    \n    # 计算权重，误分类权重调高\n    _, y_hat_label = torch.max(y_norm, 1)\n    wrong_predict_weight = (y_hat_label != y) * wrong_weight\n    wrong_predict_weight += torch.ones_like(wrong_predict_weight)\n    \n    \n    \n    T = F.one_hot(y, n_classes)\n    T = T.float()\n    right = torch.max(torch.zeros_like(y_norm), (m_plus - y_norm) * T)\n    right = right ** 2\n    wrong = torch.max(torch.zeros_like(y_norm), (y_norm - m_minus) * (1 - T))\n    wrong = _lambda * wrong ** 2\n#     loss = torch.sum((right + wrong))\n    \n    loss = torch.sum((right + wrong), dim=-1)\n    loss = torch.sum(loss * wrong_predict_weight)\n    return loss\n\n\n# 冻结网路\ndef set_parameter_requires_grad(model, feature_extracting):\n    for param in model.parameters():\n        param.requires_grad = feature_extracting\n    return 0\n\n\nclass CapsAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = CapsNet()\n        self.decoder = CNNdecoder(out_channels_dim=16)\n#         self.classifier = Caps_classifier(18)\n\n    def forward(self, x, y=None, reconstruct=False):\n        class_capsules = self.encoder(x)\n#         print(class_capsules.shape)\n        if reconstruct:\n            reconstructed_x = self.decoder(class_capsules)\n            return class_capsules, reconstructed_x\n        else:\n            return class_capsules\n#         B = x.shape[0]\n#         if reconstruct == False:\n#             return class_capsules\n#         else:\n#             if y != None:\n#                 selected_capsules = class_capsules[torch.arange(B), y]\n#             else:\n#                 class_capsules_norm = class_capsules.norm(dim=-1)\n#                 selected_capsules = class_capsules[torch.arange(B), torch.max(class_capsules_norm, dim=-1).indices]\n#             reconstructed_x = self.decoder(selected_capsules)\n#             return class_capsules, reconstructed_x","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:51:49.249729Z","iopub.execute_input":"2023-03-15T12:51:49.250110Z","iopub.status.idle":"2023-03-15T12:51:49.298019Z","shell.execute_reply.started":"2023-03-15T12:51:49.250077Z","shell.execute_reply":"2023-03-15T12:51:49.296843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# 上传之前训练好的模型数据\nsave_path = '/kaggle/working/check_points'\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n    \nresult_path = os.path.join(save_path, model_name)\n\n# import shutil\n# if os.path.exists(result_path):\n#     shutil.rmtree(result_path)\n# shutil.copytree(r'/kaggle/input/oral-cancer-dataset/models/check_points/capsnet_v2/', result_path)\n\nif not os.path.exists(result_path):\n    os.mkdir(result_path)\n\n# 读取模型数据，若不存在则初始化模型\ntorch.autograd.set_detect_anomaly(True)\nif len(glob.glob(result_path + '/**.pth')) == 0:\n    net = CapsAE()\nelse:\n    model_path = glob.glob(result_path + '/**.pth')[-1]\n    net = torch.load(model_path, map_location=device)\n\ntrain_test_data_path = os.path.join(result_path, 'train_test_data.csv')\nif os.path.exists(train_test_data_path):\n    train_test_data = pd.read_csv(train_test_data_path)\n    last_epoch = train_test_data.shape[0]\n    test_acc_best = train_test_data['test_acc_best'].values[-1]\nelse:\n    train_test_data = pd.DataFrame(data=[], columns=['train_acc', 'train_loss1', 'train_loss2', 'train_eval_loss',\n                                                     'train_lr',\n                                                     'test_acc', 'test_loss1', 'test_loss2', 'test_eval_loss',\n                                                     'epoch', 'test_acc_best'])\n    last_epoch = 0\n    test_acc_best = 0\nassert num_epoch > last_epoch, '已达训练次数'\n\n\nnet = net.to(device)\n\n# 输出模型参数与模型计算量\nflops, params = profile(net, inputs=(torch.zeros((batch_size, 3, 224, 224)).to(device),), verbose=False)\nprint(f'number of parameter: {params}', ', %.1f GFLOPS' % (flops / 1E9 * 2))\n\ndef get_parameter_number(model):\n    total_num = sum(p.numel() for p in model.parameters())\n    trainable_num = sum(p.numel() for p in model.parameters()if p.requires_grad)\n    return {'Total': total_num, 'Trainable': trainable_num}\nprint(get_parameter_number(net.encoder))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:51:50.563974Z","iopub.execute_input":"2023-03-15T12:51:50.564989Z","iopub.status.idle":"2023-03-15T12:51:57.556682Z","shell.execute_reply.started":"2023-03-15T12:51:50.564937Z","shell.execute_reply":"2023-03-15T12:51:57.555727Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"number of parameter: 4117920.0 , 66.7 GFLOPS\n{'Total': 4920736, 'Trainable': 4920736}\n","output_type":"stream"}]},{"cell_type":"code","source":"# optimizer = torch.optim.Adam([\n#     {'params': net.encoder.parameters(), 'lr': learning_rate},\n#     {'params': net.decoder.parameters(), 'lr': 0.005}\n# ])\n# 动态学习率\noptimizer = torch.optim.Adam(net.parameters(), learning_rate)\n# def lf(epoch):\n#     # 前20个epoch学习率保持不变，20个epoch后学习率按比例衰减\n#     if epoch <= 28:\n#         return 1\n#     else:\n#         lr = math.exp(0.2 * (28 - epoch))\n#         return lr\n\n# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n# scheduler.last_epoch = last_epoch - 1\n# scheduler.step()\n\ndef train(epoch, dataloder):\n    net.train()\n    t0 = time.time()\n    c = 0.00007\n    with tqdm(total=len(dataloder), file=sys.stdout) as pbar:\n        for (X_batch, y_batch) in dataloder:\n            pbar.set_description('epoch: %d' % epoch)\n            \n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            \n            y_hat = net(X_batch, reconstruct=False)\n    #         loss = margin_loss(y_batch, y_hat) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n            loss = margin_loss(y_hat, y_batch) # + c * F.mse_loss(X_batch, reconstructed_x, reduction='sum')\n    #         loss = loss_f(y_hat, y_batch) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n            loss.backward()\n            optimizer.step()\n            \n            pbar.update(1)\n    now_lr = optimizer.param_groups[0][\"lr\"]\n    t1 = time.time()\n    print(f'epoch[{epoch}] time[{round(t1-t0,1)}]s lr:{now_lr}')\n#     scheduler.step()\n    return now_lr\n\n\ndef evaluate(data_loader, type):\n    net.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        loss1 = 0\n        loss2 = 0\n        for images, labels in data_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = net(images, reconstruct=False)\n            _, predicted = torch.max(outputs.norm(dim=-1), 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            loss1 += margin_loss(outputs,labels).item()\n            #loss2 += F.mse_loss(images, reconstructed_x, reduction='sum').item()\n    eval_loss = loss1 + loss2\n    acc = 100 * correct / total\n    print(f'Accuracy on {type} set: {round(acc, 2)}%, class loss: {round(loss1, 2)}, reconstructe loss: {round(loss2, 2)}')\n    return acc, loss1 / total, loss2 / total, eval_loss / total\n\nif __name__ == '__main__':\n    weight_path_best = os.path.join(result_path, 'best')\n    if not os.path.exists(weight_path_best):\n        os.mkdir(weight_path_best)\n    for epoch in range(last_epoch, num_epoch):\n        weight_name = '{}-oral'.format(model_name)\n        lr = train(epoch, train_loader)\n        train_acc, train_loss1, train_loss2, train_eval_loss = evaluate(train_loader, type='train')\n        test_acc, test_loss1, test_loss2, test_eval_loss = evaluate(test_loader, type='test')\n        \n        # 保存训练好的模型之前，删掉已有的模型\n        for f in glob.glob(result_path + '/**.pth'):\n            os.remove(f)\n        weight_name = weight_name + '_epoch{}.pth'.format(epoch + 1)\n        weight_path = os.path.join(result_path, weight_name)\n        torch.save(net, weight_path)\n        \n        if test_acc > test_acc_best:\n            test_acc_best = test_acc\n            for f in glob.glob(weight_path_best + '/**.pth'):\n                os.remove(f)\n            torch.save(net, os.path.join(weight_path_best, weight_name))\n        train_test_data.loc[len(train_test_data.index)] = [train_acc, train_loss1, train_loss2, train_eval_loss,\n                                                           lr,\n                                                           test_acc, test_loss1, test_loss2, test_eval_loss,\n                                                           epoch, test_acc_best]\n\n        train_test_data.to_csv(train_test_data_path, index=False)\n\n    train_acc_array = train_test_data['train_acc'].values\n    test_acc_array = train_test_data['test_acc'].values\n    train_loss_array = train_test_data['train_eval_loss'].values\n    test_loss_array = train_test_data['test_eval_loss'].values\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(train_acc_array, label='Training Accuracy')\n    plt.plot(test_acc_array, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.title('Capsnet Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(train_loss_array, label='Training Loss')\n    plt.plot(test_loss_array, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Margin Entropy')\n    plt.title('Capsnet Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T12:52:47.596268Z","iopub.execute_input":"2023-03-15T12:52:47.596651Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"epoch: 0: 100%|██████████| 266/266 [03:43<00:00,  1.19it/s]\nepoch[0] time[223.7]s lr:4e-05\nAccuracy on train set: 50.02%, class loss: 2286.36, reconstructe loss: 0\nAccuracy on test set: 50.0%, class loss: 571.72, reconstructe loss: 0\nepoch: 1: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[1] time[163.3]s lr:4e-05\nAccuracy on train set: 78.03%, class loss: 1655.41, reconstructe loss: 0\nAccuracy on test set: 78.56%, class loss: 409.44, reconstructe loss: 0\nepoch: 2: 100%|██████████| 266/266 [02:42<00:00,  1.63it/s]\nepoch[2] time[162.9]s lr:4e-05\nAccuracy on train set: 80.49%, class loss: 1458.82, reconstructe loss: 0\nAccuracy on test set: 80.87%, class loss: 361.46, reconstructe loss: 0\nepoch: 3: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[3] time[163.6]s lr:4e-05\nAccuracy on train set: 75.98%, class loss: 1599.86, reconstructe loss: 0\nAccuracy on test set: 78.04%, class loss: 386.43, reconstructe loss: 0\nepoch: 4: 100%|██████████| 266/266 [02:42<00:00,  1.64it/s]\nepoch[4] time[162.6]s lr:4e-05\nAccuracy on train set: 78.64%, class loss: 1625.33, reconstructe loss: 0\nAccuracy on test set: 78.37%, class loss: 408.26, reconstructe loss: 0\nepoch: 5: 100%|██████████| 266/266 [02:42<00:00,  1.63it/s]\nepoch[5] time[163.0]s lr:4e-05\nAccuracy on train set: 78.08%, class loss: 1513.67, reconstructe loss: 0\nAccuracy on test set: 79.69%, class loss: 368.54, reconstructe loss: 0\nepoch: 6: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[6] time[163.2]s lr:4e-05\nAccuracy on train set: 78.95%, class loss: 1644.98, reconstructe loss: 0\nAccuracy on test set: 80.35%, class loss: 396.47, reconstructe loss: 0\nepoch: 7: 100%|██████████| 266/266 [02:43<00:00,  1.62it/s]\nepoch[7] time[163.8]s lr:4e-05\nAccuracy on train set: 80.73%, class loss: 1334.07, reconstructe loss: 0\nAccuracy on test set: 81.15%, class loss: 332.89, reconstructe loss: 0\nepoch: 8: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[8] time[163.2]s lr:4e-05\nAccuracy on train set: 80.63%, class loss: 1344.83, reconstructe loss: 0\nAccuracy on test set: 81.01%, class loss: 337.32, reconstructe loss: 0\nepoch: 9: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[9] time[163.1]s lr:4e-05\nAccuracy on train set: 81.58%, class loss: 1212.07, reconstructe loss: 0\nAccuracy on test set: 82.47%, class loss: 299.38, reconstructe loss: 0\nepoch: 10: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[10] time[163.1]s lr:4e-05\nAccuracy on train set: 84.3%, class loss: 1147.94, reconstructe loss: 0\nAccuracy on test set: 83.46%, class loss: 287.66, reconstructe loss: 0\nepoch: 11: 100%|██████████| 266/266 [02:42<00:00,  1.64it/s]\nepoch[11] time[162.4]s lr:4e-05\nAccuracy on train set: 83.14%, class loss: 1141.01, reconstructe loss: 0\nAccuracy on test set: 83.18%, class loss: 277.18, reconstructe loss: 0\nepoch: 12: 100%|██████████| 266/266 [02:42<00:00,  1.64it/s]\nepoch[12] time[162.5]s lr:4e-05\nAccuracy on train set: 84.65%, class loss: 1105.57, reconstructe loss: 0\nAccuracy on test set: 84.35%, class loss: 269.81, reconstructe loss: 0\nepoch: 13: 100%|██████████| 266/266 [02:42<00:00,  1.64it/s]\nepoch[13] time[162.7]s lr:4e-05\nAccuracy on train set: 82.68%, class loss: 1234.0, reconstructe loss: 0\nAccuracy on test set: 84.17%, class loss: 294.97, reconstructe loss: 0\nepoch: 14: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[14] time[163.2]s lr:4e-05\nAccuracy on train set: 88.37%, class loss: 880.68, reconstructe loss: 0\nAccuracy on test set: 88.27%, class loss: 218.41, reconstructe loss: 0\nepoch: 15: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[15] time[163.3]s lr:4e-05\nAccuracy on train set: 86.91%, class loss: 932.16, reconstructe loss: 0\nAccuracy on test set: 86.8%, class loss: 233.71, reconstructe loss: 0\nepoch: 16: 100%|██████████| 266/266 [02:42<00:00,  1.64it/s]\nepoch[16] time[162.5]s lr:4e-05\nAccuracy on train set: 89.85%, class loss: 772.87, reconstructe loss: 0\nAccuracy on test set: 89.26%, class loss: 196.27, reconstructe loss: 0\nepoch: 17: 100%|██████████| 266/266 [02:41<00:00,  1.65it/s]\nepoch[17] time[161.5]s lr:4e-05\nAccuracy on train set: 88.1%, class loss: 882.53, reconstructe loss: 0\nAccuracy on test set: 86.62%, class loss: 232.2, reconstructe loss: 0\nepoch: 18: 100%|██████████| 266/266 [02:44<00:00,  1.61it/s]\nepoch[18] time[164.9]s lr:4e-05\nAccuracy on train set: 89.95%, class loss: 753.6, reconstructe loss: 0\nAccuracy on test set: 89.59%, class loss: 193.94, reconstructe loss: 0\nepoch: 19: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[19] time[163.2]s lr:4e-05\nAccuracy on train set: 91.66%, class loss: 638.98, reconstructe loss: 0\nAccuracy on test set: 91.28%, class loss: 169.74, reconstructe loss: 0\nepoch: 20: 100%|██████████| 266/266 [02:44<00:00,  1.62it/s]\nepoch[20] time[164.4]s lr:4e-05\nAccuracy on train set: 91.59%, class loss: 634.49, reconstructe loss: 0\nAccuracy on test set: 91.05%, class loss: 162.1, reconstructe loss: 0\nepoch: 21: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[21] time[163.2]s lr:4e-05\nAccuracy on train set: 92.25%, class loss: 620.81, reconstructe loss: 0\nAccuracy on test set: 91.75%, class loss: 163.92, reconstructe loss: 0\nepoch: 22: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[22] time[163.4]s lr:4e-05\nAccuracy on train set: 90.74%, class loss: 680.23, reconstructe loss: 0\nAccuracy on test set: 90.01%, class loss: 174.46, reconstructe loss: 0\nepoch: 23: 100%|██████████| 266/266 [02:43<00:00,  1.63it/s]\nepoch[23] time[163.3]s lr:4e-05\nAccuracy on train set: 89.49%, class loss: 767.91, reconstructe loss: 0\nAccuracy on test set: 89.26%, class loss: 205.21, reconstructe loss: 0\nepoch: 24:  56%|█████▌    | 148/266 [01:31<01:11,  1.64it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}