{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        os.remove(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-26T06:40:56.065928Z","iopub.execute_input":"2023-05-26T06:40:56.066656Z","iopub.status.idle":"2023-05-26T06:40:56.087571Z","shell.execute_reply.started":"2023-05-26T06:40:56.066601Z","shell.execute_reply":"2023-05-26T06:40:56.086537Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"pip install thop","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:39:51.967188Z","iopub.execute_input":"2023-05-26T06:39:51.967857Z","iopub.status.idle":"2023-05-26T06:40:05.852008Z","shell.execute_reply.started":"2023-05-26T06:39:51.967813Z","shell.execute_reply":"2023-05-26T06:40:05.850465Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from thop) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->thop) (4.4.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport time\nfrom tqdm import tqdm\nimport sys\nimport torch.nn.functional as F\nfrom torch.nn.functional import cross_entropy\nimport os\nfrom thop import profile\nimport pandas as pd\nimport glob\nimport math\nimport torch.optim.lr_scheduler as lr_scheduler\n\n# 配置运行设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 设置参数\nbatch_size = 32\nlearning_rate = 0.00003\nnum_epoch = 50\nmodel_name = 'capsnet'\npc_out_dim = 16\nout_dim = 16\nn_classes = 2","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:40:05.855408Z","iopub.execute_input":"2023-05-26T06:40:05.856235Z","iopub.status.idle":"2023-05-26T06:40:08.615487Z","shell.execute_reply.started":"2023-05-26T06:40:05.856186Z","shell.execute_reply":"2023-05-26T06:40:08.614397Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 数据处理\nnormalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\ntransform = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor(),\n    normalize\n])\n\n# 读取图像数据\ntrain_dataset = ImageFolder('/kaggle/input/oral-cancer-dataset/Oral Cancer5/train/', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = ImageFolder('/kaggle/input/oral-cancer-dataset/Oral Cancer5/test/', transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\nprint('{0} for train. {1} for val'.format(len(train_dataset), len(test_dataset)))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:40:08.617796Z","iopub.execute_input":"2023-05-26T06:40:08.618726Z","iopub.status.idle":"2023-05-26T06:40:08.941375Z","shell.execute_reply.started":"2023-05-26T06:40:08.618683Z","shell.execute_reply":"2023-05-26T06:40:08.940324Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1680 for train. 188 for val\n","output_type":"stream"}]},{"cell_type":"code","source":"def squash(x):\n    '''\n    squash函数，保证向量长度范围在(0, 1)\n    :param x:\n        x: (B, 10, 16)\n    :return:\n        squashed x (B, 10, 16)\n    '''\n    L = torch.norm(x, dim=2, keepdim=True)  # (B, 10, 1)\n    L_square = L ** 2  # (B, 10, 1)\n    c = L_square / ((1 + L_square) * L)\n\n    s = c * x  # (B, 10, 16)\n    s[s == np.nan] = 0\n    return s\n\n\ndef dynamic_routing(x, iterations=3):\n    \"\"\"\n    动态路由\n    :param x:\n        x: (B, classes, 48*7*7, out_channels_dim, 1)\n    :param iterations:\n    :return:\n        v: next layer output (B, classes, out_channels_dim)\n    \"\"\"\n\n    N = x.shape[2]  # 输入的向量个数，此处输入32个胶囊，每个胶囊提供7*7个向量\n    N1 = x.shape[1]  # 输出类别个数\n\n\n    # 为每个向向量配置一个权重(初始为0)，并依据此权重累加向量(以类别为单位)，通过squash显示累加向量长度\n    # 计算每个初始的每个向量与累加向量的内积，来更新该向量的权重\n    b = torch.zeros(1, N1, N, 1, 1).to(x.device)  # (B, classes, 48*11*11, 1, 1)\n    for _ in range(iterations):\n        c = F.softmax(b, dim=2)  # (1, classes, 32*7*7, 1, 1)\n        s = torch.sum(x.matmul(c), dim=2).squeeze(-1)  # (B, classes, out_channels_dim)\n        v = squash(s)  # (B, classes, out_channels_dim)\n        b = b + v[:, :, None, None, :].matmul(x)\n        # (B, classes, 1, 1, out_channels_dim) .* (B, classes, 32*11*11, out_channels_dim, 1)\n        # = (B, classes, 32*11*11, 1, 1)\n    return v\n\n\nclass PrimaryCapsuleLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.primary_capsule_layer = nn.ModuleList([nn.Conv2d(512, pc_out_dim, 3, stride=1, padding=1) for _ in range(32)])\n        self.out_dim = pc_out_dim\n\n    def forward(self, x):\n        \"\"\"\n        Produce primary capsules\n        :param x:\n            x : features with # [B, 256, 14, 14]\n        :return:\n            vectors (B, 64*7*7, 128)\n        \"\"\"\n        capsules = [conv(x) for conv in self.primary_capsule_layer]  # 128 * [B, 64, 1, 1]\n        capsules_reshaped = [c.reshape(-1, self.out_dim, 7 * 7) for c in capsules]\n        s = torch.cat(capsules_reshaped, dim=-1).permute(0, 2, 1)  # (B, 128*1*1, 64)\n        return s\n\n\nclass CapsLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(1, n_classes, 32*7*7, out_dim, pc_out_dim))\n        self.classes = n_classes\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        \"\"\"\n        predict and routing\n        :param x:\n            x : input vectors, # (B, 64*14*14, 8)\n        :return:\n            class capsules, (B, 2, 64)\n        \"\"\"\n        x = x[:, None, ..., None]  # (B, 1, 32*7*7, 16, 1)\n        # W : (1, classes, 32*11*11, out_channel_dim, 16)\n        u_hat = self.W.matmul(x)  # (B, classes, 32*11*11, out_channels_dim, 1)\n        assert u_hat.shape[1:] == (self.classes, 32*7*7, self.out_dim, 1), 'dim is wrong'\n        class_capsules = dynamic_routing(u_hat, iterations=3)\n        return class_capsules\n\n\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.channels = channels\n        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n\n        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        y = self.relu(self.conv1(x))\n        y = self.conv2(y)\n        return self.relu(x + y)\n\n\nclass InceptionBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(InceptionBlock, self).__init__()\n        self.conv_s1 = torch.nn.Conv2d(channels, channels, kernel_size=1)\n        self.conv_s3 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.conv_s5 = torch.nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n        self.conv_s7 = torch.nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n        self.relu = torch.nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        y = torch.cat((self.conv_s1(x), self.conv_s3(x), self.conv_s5(x), self.conv_s7(x)), dim=1)\n        y = self.relu(y)\n        return y\n\nclass CNNdecoder(nn.Module):\n    '''\n    对输入的向量进行解码使其变为原图片\n\n\n    input_shape: [batch_size, vector_length]\n    output_shape: [batch_size, 3, 224, 224]\n\n    '''\n    def __init__(self, out_channels_dim):\n        super(CNNdecoder, self).__init__()\n        self.W = nn.Parameter(torch.randn(1, 14*14, 2))  # (1, 2, 32*12*12, out_channels_dim, 8)\n        self.upsample = nn.Upsample(scale_factor=2)\n        self.conv_layer1 = nn.Conv2d(out_channels_dim, 96, kernel_size=3, padding=1)\n        self.conv_layer2 = nn.Conv2d(96, 48, kernel_size=3, padding=1)\n        self.conv_layer3 = nn.Conv2d(48, 24, kernel_size=3, padding=1)\n        self.conv_layer4 = nn.Conv2d(24, 12, kernel_size=3, padding=1)\n        self.conv_layer5 = nn.Conv2d(12, 6, kernel_size=3, padding=1)\n        self.conv_layer6 = nn.Conv2d(6, 3, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        B = x.shape[0]\n        v_length = x.shape[2]\n#         x = x.reshape(B, 1, v_length)\n        try:\n            y = self.W.matmul(x)\n        except:\n            print('x shape:',x.shape)\n            print('W shape:',self.W.shape)                    # [B, 14*14, v_length]\n        y = y.permute(0, 2, 1).reshape((B, v_length, 14, 14))    # [B, v_length, 14, 14]\n        y = self.relu(self.conv_layer1(self.upsample(y)))  # [B, 96, 28, 28]\n        y = self.relu(self.conv_layer2(self.upsample(y)))  # [B, 48, 56, 56]\n        y = self.relu(self.conv_layer3(y))   # [B, 24, 56, 56]\n        y = self.relu(self.conv_layer4(self.upsample(y)))  # [B, 12, 112, 112]\n        y = self.relu(self.conv_layer5(self.upsample(y)))  # [B, 6, 224, 224]\n        y = self.relu(self.conv_layer6(y)) \n        return y\n\n# 解码器\nclass MLPDecoder(nn.Module):\n    \"\"\"\n    Decoder the input predicted vectors to origin images\n\n    Usages:\n        decoder = MLPDecoder([512, 1024], 16, 3, (28, 28)\n        reconstructed_x = decoder(selected_capsules)\n    \"\"\"\n    def __init__(self, hidden, in_channels, out_channels, out_shape):\n        super().__init__()\n        self.out_shape = out_shape\n        h, w = out_shape\n        self.out_channels = out_channels\n        self.out_size = h * w\n        self.mlp = nn.ModuleList()\n        for _ in range(out_channels):\n            mlp_part = nn.Sequential()\n            for i, (_in, _out) in enumerate(zip([in_channels] + hidden[:-1], hidden)):\n                mlp_part.add_module('Linear{0}'.format(i), nn.Linear(_in, _out))\n                mlp_part.add_module('Relu{0}'.format(i), nn.ReLU())\n            mlp_part.add_module('Linear{0}'.format(i+1), nn.Linear(hidden[-1], self.out_size))\n            mlp_part.add_module('Sigmoid{0}'.format(i+1), nn.Sigmoid())\n            self.mlp.append(mlp_part)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B,16)\n\n        Return:\n            reconstructed images with (B,1,28,28)\n        \"\"\"\n        B = x.shape[0]\n        output = torch.zeros((B, self.out_channels, *self.out_shape)).to(x.device)\n        for i in range(self.out_channels):\n            k = self.mlp[i]\n            out = self.mlp[i](x)\n            output[:, i, :, :] = out.reshape(B, *self.out_shape)\n        return output\n\n\n\nclass CapsNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        '''\n        input_shape: [B, 3, 224, 224]\n\n        '''\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.conv_layer1 = nn.Conv2d(3, 16, 5, stride=1, padding=2)\n        self.conv_layer2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n        self.conv_layer3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n        self.conv_layer4 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv_layer5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n        self.conv_layer6 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n        self.inception = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n        self.conv_layer_res = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n\n        \n        self.relu = nn.ReLU(inplace=True)\n        self.primary_layer = PrimaryCapsuleLayer()\n        self.caps_layer = CapsLayer()\n\n    def forward(self, x):\n        \"\"\"\n        args:\n            x: Input img, (B, 3, 224, 224)\n        return:\n            the calss capsules, ench capsule is a 16 dimension vector\n\n        \"\"\"\n        x = self.relu(self.conv_layer1(x))  # [B, 16, 224, 224]\n        x = self.maxpool(x)# [B, 16, 112, 112]\n        x = self.relu(self.conv_layer2(x))\n        y = self.maxpool(x)# [B, 32, 56, 56]\n        \n        \n        x = self.relu(self.conv_layer3(y))\n        x = self.maxpool(x) # [B, 64, 28, 28]\n        x = self.conv_layer4(x) # [B, 128, 28, 28]\n        \n        y = self.inception(y) # [B, 128, 56, 56]\n        y = self.maxpool(self.conv_layer_res(y)) # [B, 128, 28, 28]\n        \n        x = self.relu(x + y)\n        x = self.maxpool(x) # [B, 128, 14, 14]\n        x = self.relu(self.conv_layer5(x)) # [B, 256, 14, 14]\n        x = self.maxpool(x)\n        x = self.relu(self.conv_layer6(x)) # [B, 512, 7, 7]\n        x = self.primary_layer(x)  # [B, 6*6*48, 8]\n        x = self.caps_layer(x)  # (2, 48*6*6, 48, 1)\n        return x\n\n\ndef margin_loss(y_hat, y, wrong_weight=0.5):\n    '''\n    args:\n        y : ground truth labels (B)\n        y_hat : class capsules with (B, 10, 16)\n\n    return:\n        the margin loss\n    '''\n    _lambda = 0.5\n    m_plus = 0.9\n    m_minus = 0.1\n\n    y_norm = y_hat.norm(dim=-1)\n    y = y.type(torch.int64)\n    \n    # 计算权重，误分类权重调高\n    _, y_hat_label = torch.max(y_norm, 1)\n    wrong_predict_weight = (y_hat_label != y) * wrong_weight\n    wrong_predict_weight += torch.ones_like(wrong_predict_weight)\n    \n    \n    \n    T = F.one_hot(y, n_classes)\n    T = T.float()\n    right = torch.max(torch.zeros_like(y_norm), (m_plus - y_norm) * T)\n    right = right ** 2\n    wrong = torch.max(torch.zeros_like(y_norm), (y_norm - m_minus) * (1 - T))\n    wrong = _lambda * wrong ** 2\n#     loss = torch.sum((right + wrong))\n    \n    loss = torch.sum((right + wrong), dim=-1)\n    loss = torch.sum(loss * wrong_predict_weight)\n    return loss\n\n\n# 冻结网路\ndef set_parameter_requires_grad(model, feature_extracting):\n    for param in model.parameters():\n        param.requires_grad = feature_extracting\n    return 0\n\n\nclass CapsAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = CapsNet()\n        self.decoder = CNNdecoder(out_channels_dim=16)\n#         self.classifier = Caps_classifier(18)\n\n    def forward(self, x, y=None, reconstruct=False):\n        class_capsules = self.encoder(x)\n#         print(class_capsules.shape)\n        if reconstruct:\n            reconstructed_x = self.decoder(class_capsules)\n            return class_capsules, reconstructed_x\n        else:\n            return class_capsules\n#         B = x.shape[0]\n#         if reconstruct == False:\n#             return class_capsules\n#         else:\n#             if y != None:\n#                 selected_capsules = class_capsules[torch.arange(B), y]\n#             else:\n#                 class_capsules_norm = class_capsules.norm(dim=-1)\n#                 selected_capsules = class_capsules[torch.arange(B), torch.max(class_capsules_norm, dim=-1).indices]\n#             reconstructed_x = self.decoder(selected_capsules)\n#             return class_capsules, reconstructed_x","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:40:22.685615Z","iopub.execute_input":"2023-05-26T06:40:22.686506Z","iopub.status.idle":"2023-05-26T06:40:22.756473Z","shell.execute_reply.started":"2023-05-26T06:40:22.686465Z","shell.execute_reply":"2023-05-26T06:40:22.755002Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def freeze_frature_net(model):\n    # 冻结所有参数\n    for p in model.parameters():\n        p.requires_grad = False\n    # 解冻primary_layer和caps_layer\n    for p in net.encoder.primary_layer.parameters():\n        p.requires_grad = True\n    for p in model.encoder.caps_layer.parameters():\n        p.requires_grad = True\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:40:23.428868Z","iopub.execute_input":"2023-05-26T06:40:23.429596Z","iopub.status.idle":"2023-05-26T06:40:23.436657Z","shell.execute_reply.started":"2023-05-26T06:40:23.429555Z","shell.execute_reply":"2023-05-26T06:40:23.435292Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pre_trained_model_path = '/kaggle/input/oral-cancer-dataset/breakhis/capsnet/best/capsnet-oral_epoch46.pth'\n\n\n# 上传之前训练好的模型数据\nsave_path = '/kaggle/working/check_points'\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n    \nresult_path = os.path.join(save_path, model_name)\n\n# import shutil\n# if os.path.exists(result_path):\n#     shutil.rmtree(result_path)\n# shutil.copytree(r'/kaggle/input/oral-cancer-dataset/models/check_points/capsnet_v2/', result_path)\n\nif not os.path.exists(result_path):\n    os.mkdir(result_path)\n\n# 读取模型数据，若不存在则初始化模型\ntorch.autograd.set_detect_anomaly(True)\nif len(glob.glob(result_path + '/**.pth')) == 0:\n    net = torch.load(pre_trained_model_path, map_location=device)\nelse:\n    model_path = glob.glob(result_path + '/**.pth')[-1]\n    net = torch.load(model_path, map_location=device)\n    \ntrain_test_data_path = os.path.join(result_path, 'train_test_data.csv')\nif os.path.exists(train_test_data_path):\n    train_test_data = pd.read_csv(train_test_data_path)\n    last_epoch = train_test_data.shape[0]\n    test_acc_best = train_test_data['test_acc_best'].values[-1]\nelse:\n    train_test_data = pd.DataFrame(data=[], columns=['train_acc', 'train_loss1', 'train_loss2', 'train_eval_loss',\n                                                     'train_lr',\n                                                     'test_acc', 'test_loss1', 'test_loss2', 'test_eval_loss',\n                                                     'epoch', 'test_acc_best'])\n    last_epoch = 0\n    test_acc_best = 0\nassert num_epoch > last_epoch, '已达训练次数'\n\n\nnet = net.to(device)\n\n# 输出模型参数与模型计算量\nflops, params = profile(net, inputs=(torch.zeros((batch_size, 3, 224, 224)).to(device),), verbose=False)\nprint(f'number of parameter: {params}', ', %.1f GFLOPS' % (flops / 1E9 * 2))\n\ndef get_parameter_number(model):\n    total_num = sum(p.numel() for p in model.parameters())\n    trainable_num = sum(p.numel() for p in model.parameters()if p.requires_grad)\n    return {'Total': total_num, 'Trainable': trainable_num}\nprint(get_parameter_number(net.encoder))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:41:03.270608Z","iopub.execute_input":"2023-05-26T06:41:03.271347Z","iopub.status.idle":"2023-05-26T06:41:07.842109Z","shell.execute_reply.started":"2023-05-26T06:41:03.271307Z","shell.execute_reply":"2023-05-26T06:41:07.840878Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"number of parameter: 4117920.0 , 66.7 GFLOPS\n{'Total': 4920736, 'Trainable': 4920736}\n","output_type":"stream"}]},{"cell_type":"code","source":"# optimizer = torch.optim.Adam([\n#     {'params': net.encoder.parameters(), 'lr': learning_rate},\n#     {'params': net.decoder.parameters(), 'lr': 0.005}\n# ])\n# 动态学习率\noptimizer = torch.optim.Adam(net.parameters(), learning_rate)\n# def lf(epoch):\n#     # 前20个epoch学习率保持不变，20个epoch后学习率按比例衰减\n#     if epoch <= 28:\n#         return 1\n#     else:\n#         lr = math.exp(0.2 * (28 - epoch))\n#         return lr\n\n# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n# scheduler.last_epoch = last_epoch - 1\n# scheduler.step()\n\ndef train(epoch, dataloder):\n    net.train()\n    t0 = time.time()\n    c = 0.00007\n    with tqdm(total=len(dataloder), file=sys.stdout) as pbar:\n        for (X_batch, y_batch) in dataloder:\n            pbar.set_description('epoch: %d' % epoch)\n            \n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            \n            y_hat = net(X_batch, reconstruct=False)\n    #         loss = margin_loss(y_batch, y_hat) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n            loss = margin_loss(y_hat, y_batch) # + c * F.mse_loss(X_batch, reconstructed_x, reduction='sum')\n    #         loss = loss_f(y_hat, y_batch) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n            loss.backward()\n            optimizer.step()\n            \n            pbar.update(1)\n    now_lr = optimizer.param_groups[0][\"lr\"]\n    t1 = time.time()\n    print(f'epoch[{epoch}] time[{round(t1-t0,1)}]s lr:{now_lr}')\n#     scheduler.step()\n    return now_lr\n\n\ndef evaluate(data_loader, type):\n    net.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        loss1 = 0\n        loss2 = 0\n        for images, labels in data_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = net(images, reconstruct=False)\n            _, predicted = torch.max(outputs.norm(dim=-1), 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            loss1 += margin_loss(outputs,labels).item()\n            #loss2 += F.mse_loss(images, reconstructed_x, reduction='sum').item()\n    eval_loss = loss1 + loss2\n    acc = 100 * correct / total\n    print(f'Accuracy on {type} set: {round(acc, 2)}%, class loss: {round(loss1, 2)}, reconstructe loss: {round(loss2, 2)}')\n    return acc, loss1 / total, loss2 / total, eval_loss / total\n\nif __name__ == '__main__':\n    weight_path_best = os.path.join(result_path, 'best')\n    if not os.path.exists(weight_path_best):\n        os.mkdir(weight_path_best)\n    for epoch in range(last_epoch, num_epoch):\n        weight_name = '{}-oral'.format(model_name)\n        lr = train(epoch, train_loader)\n        train_acc, train_loss1, train_loss2, train_eval_loss = evaluate(train_loader, type='train')\n        test_acc, test_loss1, test_loss2, test_eval_loss = evaluate(test_loader, type='test')\n        \n        # 保存训练好的模型之前，删掉已有的模型\n        for f in glob.glob(result_path + '/**.pth'):\n            os.remove(f)\n        weight_name = weight_name + '_epoch{}.pth'.format(epoch + 1)\n        weight_path = os.path.join(result_path, weight_name)\n        torch.save(net, weight_path)\n        \n        if test_acc > test_acc_best:\n            test_acc_best = test_acc\n            for f in glob.glob(weight_path_best + '/**.pth'):\n                os.remove(f)\n            torch.save(net, os.path.join(weight_path_best, weight_name))\n        train_test_data.loc[len(train_test_data.index)] = [train_acc, train_loss1, train_loss2, train_eval_loss,\n                                                           lr,\n                                                           test_acc, test_loss1, test_loss2, test_eval_loss,\n                                                           epoch, test_acc_best]\n\n        train_test_data.to_csv(train_test_data_path, index=False)\n\n    train_acc_array = train_test_data['train_acc'].values\n    test_acc_array = train_test_data['test_acc'].values\n    train_loss_array = train_test_data['train_eval_loss'].values\n    test_loss_array = train_test_data['test_eval_loss'].values\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(train_acc_array, label='Training Accuracy')\n    plt.plot(test_acc_array, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.title('Capsnet Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(train_loss_array, label='Training Loss')\n    plt.plot(test_loss_array, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Margin Entropy')\n    plt.title('Capsnet Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T06:41:15.912408Z","iopub.execute_input":"2023-05-26T06:41:15.912918Z","iopub.status.idle":"2023-05-26T06:41:25.803146Z","shell.execute_reply.started":"2023-05-26T06:41:15.912872Z","shell.execute_reply":"2023-05-26T06:41:25.801341Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"epoch: 0:   4%|▍         | 2/53 [00:09<04:09,  4.90s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/3719365090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mweight_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}-oral'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_eval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/3719365090.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, dataloder)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \"\"\"\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}