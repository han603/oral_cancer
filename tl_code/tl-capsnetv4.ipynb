{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-22T06:54:32.954748Z",
     "iopub.status.busy": "2023-03-22T06:54:32.954295Z",
     "iopub.status.idle": "2023-03-22T06:54:32.981503Z",
     "shell.execute_reply": "2023-03-22T06:54:32.980231Z",
     "shell.execute_reply.started": "2023-03-22T06:54:32.954711Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        os.remove(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T08:38:55.172519Z",
     "iopub.status.busy": "2023-03-22T08:38:55.172083Z",
     "iopub.status.idle": "2023-03-22T08:39:05.968690Z",
     "shell.execute_reply": "2023-03-22T08:39:05.967427Z",
     "shell.execute_reply.started": "2023-03-22T08:38:55.172481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from thop) (1.13.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->thop) (4.4.0)\n",
      "Installing collected packages: thop\n",
      "Successfully installed thop-0.1.1.post2209072238\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T11:27:29.779444Z",
     "iopub.status.busy": "2023-03-22T11:27:29.778739Z",
     "iopub.status.idle": "2023-03-22T11:27:29.786531Z",
     "shell.execute_reply": "2023-03-22T11:27:29.785491Z",
     "shell.execute_reply.started": "2023-03-22T11:27:29.779408Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "import os\n",
    "from thop import profile\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# 配置运行设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 设置参数\n",
    "batch_size = 32\n",
    "learning_rate = 0.00003\n",
    "num_epoch = 50\n",
    "model_name = 'capsnet_v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T08:39:08.548943Z",
     "iopub.status.busy": "2023-03-22T08:39:08.548329Z",
     "iopub.status.idle": "2023-03-22T08:39:08.825884Z",
     "shell.execute_reply": "2023-03-22T08:39:08.824790Z",
     "shell.execute_reply.started": "2023-03-22T08:39:08.548912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 for train. 188 for val\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "normalize = transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# 读取图像数据\n",
    "train_dataset = ImageFolder('/kaggle/input/oral-cancer-dataset/Oral Cancer5/train/', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = ImageFolder('/kaggle/input/oral-cancer-dataset/Oral Cancer5/test/', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('{0} for train. {1} for val'.format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T11:27:32.500268Z",
     "iopub.status.busy": "2023-03-22T11:27:32.499893Z",
     "iopub.status.idle": "2023-03-22T11:27:32.553244Z",
     "shell.execute_reply": "2023-03-22T11:27:32.552070Z",
     "shell.execute_reply.started": "2023-03-22T11:27:32.500230Z"
    }
   },
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    '''\n",
    "    squash函数，保证向量长度范围在(0, 1)\n",
    "    :param x:\n",
    "        x: (B, 10, 16)\n",
    "    :return:\n",
    "        squashed x (B, 10, 16)\n",
    "    '''\n",
    "    L = torch.norm(x, dim=2, keepdim=True)  # (B, 10, 1)\n",
    "    L_square = L ** 2  # (B, 10, 1)\n",
    "    c = L_square / ((1 + L_square) * L)\n",
    "\n",
    "    s = c * x  # (B, 10, 16)\n",
    "    s[s == np.nan] = 0\n",
    "    return s\n",
    "\n",
    "\n",
    "def dynamic_routing(x, iterations=3):\n",
    "    \"\"\"\n",
    "    动态路由\n",
    "    :param x:\n",
    "        x: (B, classes, 48*7*7, out_channels_dim, 1)\n",
    "    :param iterations:\n",
    "    :return:\n",
    "        v: next layer output (B, classes, out_channels_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    N = x.shape[2]  # 输入的向量个数，此处输入48个胶囊，每个胶囊提供7*7个向量\n",
    "    N1 = x.shape[1]  # 输出类别个数\n",
    "    B = x.shape[0]  # batch_size\n",
    "\n",
    "    # 为每个向向量配置一个权重(初始为0)，并依据此权重累加向量(以类别为单位)，通过squash显示累加向量长度\n",
    "    # 计算每个初始的每个向量与累加向量的内积，来更新该向量的权重\n",
    "    b = torch.zeros(B, N1, N, 1, 1).to(x.device)  # (B, classes, 48*11*11, 1, 1)\n",
    "    for _ in range(iterations):\n",
    "        c = F.softmax(b, dim=1)  # (B, classes, 48*11*11, 1, 1)\n",
    "        s = torch.sum(x.matmul(c), dim=2).squeeze(-1)  # (B, classes, out_channels_dim)\n",
    "        v = F.softmax(s, dim=-1)  # (B, classes, out_channels_dim)\n",
    "        b = b + v[:, :, None, None, :].matmul(x)\n",
    "        # (B, classes, 1, 1, out_channels_dim) .* (B, classes, 32*11*11, out_channels_dim, 1)\n",
    "        # = (B, classes, 32*11*11, 1, 1)\n",
    "    return v\n",
    "\n",
    "class dynamic(nn.Module):\n",
    "    def __init__(self, N1, N):\n",
    "        super().__init__()\n",
    "        self.b = nn.Parameter(torch.randn(1, N1, N, 1, 1), requires_grad=True)\n",
    "        self.N1 = N1\n",
    "        self.N = N\n",
    "        \n",
    "    def dynamic_routing(self, x):\n",
    "        \"\"\"\n",
    "        动态路由\n",
    "        :param x:\n",
    "            x: (B, classes, 48*7*7, out_channels_dim, 1)\n",
    "        :param iterations:\n",
    "        :return:\n",
    "            v: next layer output (B, classes, out_channels_dim)\n",
    "        \"\"\"\n",
    "        c = F.softmax(self.b, dim=1)  # (B, classes, 48*11*11, 1, 1)\n",
    "        assert c.shape == self.b.shape, f\"dim is change{c.shape}\"\n",
    "        s = torch.sum(x.matmul(c), dim=2).squeeze(-1)  # (B, classes, out_channels_dim)\n",
    "        v = squash(s)  # (B, classes, out_channels_dim)\n",
    "            # (B, classes, 1, 1, out_channels_dim) .* (B, classes, 32*11*11, out_channels_dim, 1)\n",
    "            # = (B, classes, 32*11*11, 1, 1)\n",
    "        return v\n",
    "        \n",
    "\n",
    "\n",
    "class PrimaryCapsuleLayer(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.primary_capsule_layer = nn.ModuleList([nn.Conv2d(512, out_dim, 3, stride=1, padding=1) for _ in range(32)])\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Produce primary capsules\n",
    "        :param x:\n",
    "            x : features with # [B, 256, 14, 14]\n",
    "        :return:\n",
    "            vectors (B, 64*7*7, 128)\n",
    "        \"\"\"\n",
    "        capsules = [conv(x) for conv in self.primary_capsule_layer]  # 128 * [B, 64, 1, 1]\n",
    "        capsules_reshaped = [c.reshape(-1, self.out_dim, 7 * 7) for c in capsules]\n",
    "        s = torch.cat(capsules_reshaped, dim=-1).permute(0, 2, 1)  # (B, 128*1*1, 64)\n",
    "        return s\n",
    "\n",
    "\n",
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.dynamic = dynamic(2, 32 * 7 * 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        predict and routing\n",
    "        :param x:\n",
    "            x : input vectors, # (B, 64*14*14, 8)\n",
    "        :return:\n",
    "            class capsules, (B, 2, 64)\n",
    "        \"\"\"\n",
    "        x = x[:, None, ..., None]\n",
    "\n",
    "        class_capsules = self.dynamic.dynamic_routing(x)\n",
    "        return class_capsules\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return self.relu(x + y)\n",
    "\n",
    "\n",
    "class InceptionBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.conv_s1 = torch.nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.conv_s3 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv_s5 = torch.nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n",
    "        self.conv_s7 = torch.nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.cat((self.conv_s1(x), self.conv_s3(x), self.conv_s5(x), self.conv_s7(x)), dim=1)\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "class CNNdecoder(nn.Module):\n",
    "    '''\n",
    "    对输入的向量进行解码使其变为原图片\n",
    "\n",
    "\n",
    "    input_shape: [batch_size, vector_length]\n",
    "    output_shape: [batch_size, 3, 224, 224]\n",
    "\n",
    "    '''\n",
    "    def __init__(self, out_channels_dim):\n",
    "        super(CNNdecoder, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(1, 14*14, 2))  # (1, 2, 32*12*12, out_channels_dim, 8)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv_layer1 = nn.Conv2d(out_channels_dim, 96, kernel_size=3, padding=1)\n",
    "        self.conv_layer2 = nn.Conv2d(96, 48, kernel_size=3, padding=1)\n",
    "        self.conv_layer3 = nn.Conv2d(48, 24, kernel_size=3, padding=1)\n",
    "        self.conv_layer4 = nn.Conv2d(24, 12, kernel_size=3, padding=1)\n",
    "        self.conv_layer5 = nn.Conv2d(12, 6, kernel_size=3, padding=1)\n",
    "        self.conv_layer6 = nn.Conv2d(6, 3, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        v_length = x.shape[2]\n",
    "#         x = x.reshape(B, 1, v_length)\n",
    "        try:\n",
    "            y = self.W.matmul(x)\n",
    "        except:\n",
    "            print('x shape:',x.shape)\n",
    "            print('W shape:',self.W.shape)                    # [B, 14*14, v_length]\n",
    "        y = y.permute(0, 2, 1).reshape((B, v_length, 14, 14))    # [B, v_length, 14, 14]\n",
    "        y = self.relu(self.conv_layer1(self.upsample(y)))  # [B, 96, 28, 28]\n",
    "        y = self.relu(self.conv_layer2(self.upsample(y)))  # [B, 48, 56, 56]\n",
    "        y = self.relu(self.conv_layer3(y))   # [B, 24, 56, 56]\n",
    "        y = self.relu(self.conv_layer4(self.upsample(y)))  # [B, 12, 112, 112]\n",
    "        y = self.relu(self.conv_layer5(self.upsample(y)))  # [B, 6, 224, 224]\n",
    "        y = self.relu(self.conv_layer6(y)) \n",
    "        return y\n",
    "\n",
    "# 解码器\n",
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder the input predicted vectors to origin images\n",
    "\n",
    "    Usages:\n",
    "        decoder = MLPDecoder([512, 1024], 16, 3, (28, 28)\n",
    "        reconstructed_x = decoder(selected_capsules)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, in_channels, out_channels, out_shape):\n",
    "        super().__init__()\n",
    "        self.out_shape = out_shape\n",
    "        h, w = out_shape\n",
    "        self.out_channels = out_channels\n",
    "        self.out_size = h * w\n",
    "        self.mlp = nn.ModuleList()\n",
    "        for _ in range(out_channels):\n",
    "            mlp_part = nn.Sequential()\n",
    "            for i, (_in, _out) in enumerate(zip([in_channels] + hidden[:-1], hidden)):\n",
    "                mlp_part.add_module('Linear{0}'.format(i), nn.Linear(_in, _out))\n",
    "                mlp_part.add_module('Relu{0}'.format(i), nn.ReLU())\n",
    "            mlp_part.add_module('Linear{0}'.format(i+1), nn.Linear(hidden[-1], self.out_size))\n",
    "            mlp_part.add_module('Sigmoid{0}'.format(i+1), nn.Sigmoid())\n",
    "            self.mlp.append(mlp_part)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B,16)\n",
    "\n",
    "        Return:\n",
    "            reconstructed images with (B,1,28,28)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        output = torch.zeros((B, self.out_channels, *self.out_shape)).to(x.device)\n",
    "        for i in range(self.out_channels):\n",
    "            k = self.mlp[i]\n",
    "            out = self.mlp[i](x)\n",
    "            output[:, i, :, :] = out.reshape(B, *self.out_shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, out_vector_dims):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        input_shape: [B, 3, 224, 224]\n",
    "\n",
    "        '''\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(3, 16, 5, stride=1, padding=2)\n",
    "        self.conv_layer2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        self.conv_layer3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        self.conv_layer4 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.conv_layer5 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.conv_layer6 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.inception = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
    "        self.conv_layer_res = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.primary_layer = PrimaryCapsuleLayer(out_dim=out_vector_dims)\n",
    "        self.caps_layer = CapsLayer(classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: Input img, (B, 3, 224, 224)\n",
    "        return:\n",
    "            the calss capsules, ench capsule is a 16 dimension vector\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.relu(self.conv_layer1(x))  # [B, 16, 224, 224]\n",
    "        x = self.maxpool(x)# [B, 16, 112, 112]\n",
    "        x = self.relu(self.conv_layer2(x))\n",
    "        y = self.maxpool(x)# [B, 32, 56, 56]\n",
    "        \n",
    "        \n",
    "        x = self.relu(self.conv_layer3(y))\n",
    "        x = self.maxpool(x) # [B, 64, 28, 28]\n",
    "        x = self.conv_layer4(x) # [B, 128, 28, 28]\n",
    "        \n",
    "        y = self.inception(y) # [B, 128, 56, 56]\n",
    "        y = self.maxpool(self.conv_layer_res(y)) # [B, 128, 28, 28]\n",
    "        \n",
    "        x = self.relu(x + y)\n",
    "        x = self.maxpool(x) # [B, 128, 14, 14]\n",
    "        x = self.relu(self.conv_layer5(x)) # [B, 256, 14, 14]\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv_layer6(x)) # [B, 512, 7, 7]\n",
    "        x = self.primary_layer(x)  # [B, 6*6*48, 8]\n",
    "        x = self.caps_layer(x)  # (2, 48*6*6, 48, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def margin_loss(y_hat, y, wrong_weight=0.5):\n",
    "    '''\n",
    "    args:\n",
    "        y : ground truth labels (B)\n",
    "        y_hat : class capsules with (B, 10, 16)\n",
    "\n",
    "    return:\n",
    "        the margin loss\n",
    "    '''\n",
    "    _lambda = 0.5\n",
    "    m_plus = 0.9\n",
    "    m_minus = 0.1\n",
    "    nclass = 2\n",
    "\n",
    "    y_norm = y_hat.norm(dim=-1)\n",
    "    y = y.type(torch.int64)\n",
    "    \n",
    "    # 计算权重，误分类权重调高\n",
    "    _, y_hat_label = torch.max(y_norm, 1)\n",
    "    wrong_predict_weight = (y_hat_label != y) * wrong_weight\n",
    "    wrong_predict_weight += torch.ones_like(wrong_predict_weight)\n",
    "    \n",
    "    \n",
    "    \n",
    "    T = F.one_hot(y, nclass)\n",
    "    T = T.float()\n",
    "    right = torch.max(torch.zeros_like(y_norm), (m_plus - y_norm) * T)\n",
    "    right = right ** 2\n",
    "    wrong = torch.max(torch.zeros_like(y_norm), (y_norm - m_minus) * (1 - T))\n",
    "    wrong = _lambda * wrong ** 2\n",
    "#     loss = torch.sum((right + wrong))\n",
    "    \n",
    "    loss = torch.sum((right + wrong), dim=-1)\n",
    "    loss = torch.sum(loss * wrong_predict_weight)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 冻结网路\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = feature_extracting\n",
    "    return 0\n",
    "\n",
    "\n",
    "class CapsAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CapsNet(out_vector_dims=16)\n",
    "        self.decoder = CNNdecoder(out_channels_dim=16)\n",
    "#         self.classifier = Caps_classifier(18)\n",
    "\n",
    "    def forward(self, x, y=None, reconstruct=False):\n",
    "        class_capsules = self.encoder(x)\n",
    "#         print(class_capsules.shape)\n",
    "        if reconstruct:\n",
    "            reconstructed_x = self.decoder(class_capsules)\n",
    "            return class_capsules, reconstructed_x\n",
    "        else:\n",
    "            return class_capsules\n",
    "#         B = x.shape[0]\n",
    "#         if reconstruct == False:\n",
    "#             return class_capsules\n",
    "#         else:\n",
    "#             if y != None:\n",
    "#                 selected_capsules = class_capsules[torch.arange(B), y]\n",
    "#             else:\n",
    "#                 class_capsules_norm = class_capsules.norm(dim=-1)\n",
    "#                 selected_capsules = class_capsules[torch.arange(B), torch.max(class_capsules_norm, dim=-1).indices]\n",
    "#             reconstructed_x = self.decoder(selected_capsules)\n",
    "#             return class_capsules, reconstructed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T11:27:34.072422Z",
     "iopub.status.busy": "2023-03-22T11:27:34.071702Z",
     "iopub.status.idle": "2023-03-22T11:27:34.130747Z",
     "shell.execute_reply": "2023-03-22T11:27:34.129175Z",
     "shell.execute_reply.started": "2023-03-22T11:27:34.072384Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "已达训练次数",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/1585146341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlast_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtest_acc_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'已达训练次数'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 已达训练次数"
     ]
    }
   ],
   "source": [
    "# 上传之前训练好的模型数据\n",
    "save_path = '/kaggle/working/check_points'\n",
    "pretrained_model_path = '/kaggle/input/oral-cancer-dataset/breakhis/capsnet_v4/best/capsnet_v4-oral_epoch59.pth'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "result_path = os.path.join(save_path, model_name)\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.mkdir(result_path)\n",
    "\n",
    "# 读取模型数据，若不存在则初始化模型\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "if len(glob.glob(result_path + '/**.pth')) == 0:\n",
    "    net = torch.load(pretrained_model_path, map_location=device)\n",
    "else:\n",
    "    model_path = glob.glob(result_path + '/**.pth')[-1]\n",
    "    net = torch.load(model_path, map_location=device)\n",
    "\n",
    "train_test_data_path = os.path.join(result_path, 'train_test_data.csv')\n",
    "if os.path.exists(train_test_data_path):\n",
    "    train_test_data = pd.read_csv(train_test_data_path)\n",
    "    last_epoch = train_test_data.shape[0]\n",
    "    test_acc_best = train_test_data['test_acc_best'].values[-1]\n",
    "else:\n",
    "    train_test_data = pd.DataFrame(data=[], columns=['train_acc', 'train_loss1', 'train_loss2', 'train_eval_loss',\n",
    "                                                     'train_lr',\n",
    "                                                     'test_acc', 'test_loss1', 'test_loss2', 'test_eval_loss',\n",
    "                                                     'epoch', 'test_acc_best'])\n",
    "    last_epoch = 0\n",
    "    test_acc_best = 0\n",
    "assert num_epoch > last_epoch, '已达训练次数'\n",
    "\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# 输出模型参数与模型计算量\n",
    "flops, params = profile(net, inputs=(torch.zeros((batch_size, 3, 224, 224)).to(device),), verbose=False)\n",
    "print(f'number of parameter: {params}', ', %.1f GFLOPS' % (flops / 1E9 * 2))\n",
    "\n",
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters()if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}\n",
    "print(get_parameter_number(net.encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T08:39:15.782912Z",
     "iopub.status.busy": "2023-03-22T08:39:15.782307Z",
     "iopub.status.idle": "2023-03-22T11:27:13.375058Z",
     "shell.execute_reply": "2023-03-22T11:27:13.373100Z",
     "shell.execute_reply.started": "2023-03-22T08:39:15.782871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18: 100%|██████████| 53/53 [03:04<00:00,  3.48s/it]\n",
      "epoch[18] time[184.6]s lr:3e-05\n",
      "Accuracy on train set: 96.61%, class loss: 63.46, reconstructe loss: 0\n",
      "Accuracy on test set: 92.55%, class loss: 16.59, reconstructe loss: 0\n",
      "epoch: 19: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[19] time[153.1]s lr:3e-05\n",
      "Accuracy on train set: 98.69%, class loss: 41.51, reconstructe loss: 0\n",
      "Accuracy on test set: 94.15%, class loss: 18.69, reconstructe loss: 0\n",
      "epoch: 20: 100%|██████████| 53/53 [02:32<00:00,  2.89s/it]\n",
      "epoch[20] time[152.9]s lr:3e-05\n",
      "Accuracy on train set: 98.57%, class loss: 35.15, reconstructe loss: 0\n",
      "Accuracy on test set: 93.62%, class loss: 15.53, reconstructe loss: 0\n",
      "epoch: 21: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[21] time[153.2]s lr:3e-05\n",
      "Accuracy on train set: 98.81%, class loss: 29.84, reconstructe loss: 0\n",
      "Accuracy on test set: 93.09%, class loss: 15.86, reconstructe loss: 0\n",
      "epoch: 22: 100%|██████████| 53/53 [02:30<00:00,  2.83s/it]\n",
      "epoch[22] time[150.0]s lr:3e-05\n",
      "Accuracy on train set: 98.51%, class loss: 39.28, reconstructe loss: 0\n",
      "Accuracy on test set: 88.83%, class loss: 21.7, reconstructe loss: 0\n",
      "epoch: 23: 100%|██████████| 53/53 [02:29<00:00,  2.83s/it]\n",
      "epoch[23] time[149.8]s lr:3e-05\n",
      "Accuracy on train set: 98.45%, class loss: 34.58, reconstructe loss: 0\n",
      "Accuracy on test set: 90.96%, class loss: 19.14, reconstructe loss: 0\n",
      "epoch: 24: 100%|██████████| 53/53 [02:30<00:00,  2.83s/it]\n",
      "epoch[24] time[150.2]s lr:3e-05\n",
      "Accuracy on train set: 98.99%, class loss: 28.38, reconstructe loss: 0\n",
      "Accuracy on test set: 93.09%, class loss: 15.94, reconstructe loss: 0\n",
      "epoch: 25: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[25] time[149.3]s lr:3e-05\n",
      "Accuracy on train set: 98.27%, class loss: 37.51, reconstructe loss: 0\n",
      "Accuracy on test set: 94.68%, class loss: 12.46, reconstructe loss: 0\n",
      "epoch: 26: 100%|██████████| 53/53 [02:29<00:00,  2.83s/it]\n",
      "epoch[26] time[150.0]s lr:3e-05\n",
      "Accuracy on train set: 98.75%, class loss: 30.61, reconstructe loss: 0\n",
      "Accuracy on test set: 89.89%, class loss: 19.7, reconstructe loss: 0\n",
      "epoch: 27: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[27] time[153.3]s lr:3e-05\n",
      "Accuracy on train set: 98.51%, class loss: 31.48, reconstructe loss: 0\n",
      "Accuracy on test set: 92.55%, class loss: 16.78, reconstructe loss: 0\n",
      "epoch: 28: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[28] time[153.2]s lr:3e-05\n",
      "Accuracy on train set: 98.93%, class loss: 31.43, reconstructe loss: 0\n",
      "Accuracy on test set: 90.43%, class loss: 21.09, reconstructe loss: 0\n",
      "epoch: 29: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[29] time[153.2]s lr:3e-05\n",
      "Accuracy on train set: 99.05%, class loss: 20.53, reconstructe loss: 0\n",
      "Accuracy on test set: 94.15%, class loss: 14.21, reconstructe loss: 0\n",
      "epoch: 30: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[30] time[149.2]s lr:3e-05\n",
      "Accuracy on train set: 98.81%, class loss: 28.59, reconstructe loss: 0\n",
      "Accuracy on test set: 92.55%, class loss: 15.37, reconstructe loss: 0\n",
      "epoch: 31: 100%|██████████| 53/53 [02:29<00:00,  2.83s/it]\n",
      "epoch[31] time[149.8]s lr:3e-05\n",
      "Accuracy on train set: 99.11%, class loss: 18.75, reconstructe loss: 0\n",
      "Accuracy on test set: 92.02%, class loss: 14.3, reconstructe loss: 0\n",
      "epoch: 32: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[32] time[149.7]s lr:3e-05\n",
      "Accuracy on train set: 98.63%, class loss: 31.26, reconstructe loss: 0\n",
      "Accuracy on test set: 94.68%, class loss: 14.6, reconstructe loss: 0\n",
      "epoch: 33: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[33] time[149.5]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 17.3, reconstructe loss: 0\n",
      "Accuracy on test set: 92.55%, class loss: 12.94, reconstructe loss: 0\n",
      "epoch: 34: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[34] time[149.5]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 18.63, reconstructe loss: 0\n",
      "Accuracy on test set: 92.02%, class loss: 15.28, reconstructe loss: 0\n",
      "epoch: 35: 100%|██████████| 53/53 [02:29<00:00,  2.82s/it]\n",
      "epoch[35] time[149.7]s lr:3e-05\n",
      "Accuracy on train set: 98.99%, class loss: 21.76, reconstructe loss: 0\n",
      "Accuracy on test set: 92.02%, class loss: 15.97, reconstructe loss: 0\n",
      "epoch: 36: 100%|██████████| 53/53 [02:29<00:00,  2.83s/it]\n",
      "epoch[36] time[149.9]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 15.29, reconstructe loss: 0\n",
      "Accuracy on test set: 93.09%, class loss: 14.26, reconstructe loss: 0\n",
      "epoch: 37: 100%|██████████| 53/53 [02:32<00:00,  2.89s/it]\n",
      "epoch[37] time[152.9]s lr:3e-05\n",
      "Accuracy on train set: 99.11%, class loss: 18.19, reconstructe loss: 0\n",
      "Accuracy on test set: 92.02%, class loss: 15.06, reconstructe loss: 0\n",
      "epoch: 38: 100%|██████████| 53/53 [02:32<00:00,  2.89s/it]\n",
      "epoch[38] time[153.0]s lr:3e-05\n",
      "Accuracy on train set: 98.99%, class loss: 19.8, reconstructe loss: 0\n",
      "Accuracy on test set: 88.3%, class loss: 20.29, reconstructe loss: 0\n",
      "epoch: 39: 100%|██████████| 53/53 [02:33<00:00,  2.89s/it]\n",
      "epoch[39] time[153.3]s lr:3e-05\n",
      "Accuracy on train set: 99.11%, class loss: 33.9, reconstructe loss: 0\n",
      "Accuracy on test set: 90.96%, class loss: 16.8, reconstructe loss: 0\n",
      "epoch: 40: 100%|██████████| 53/53 [02:29<00:00,  2.83s/it]\n",
      "epoch[40] time[149.9]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 14.6, reconstructe loss: 0\n",
      "Accuracy on test set: 93.09%, class loss: 13.84, reconstructe loss: 0\n",
      "epoch: 41: 100%|██████████| 53/53 [02:29<00:00,  2.81s/it]\n",
      "epoch[41] time[149.0]s lr:3e-05\n",
      "Accuracy on train set: 98.93%, class loss: 30.22, reconstructe loss: 0\n",
      "Accuracy on test set: 82.98%, class loss: 24.17, reconstructe loss: 0\n",
      "epoch: 42: 100%|██████████| 53/53 [02:30<00:00,  2.83s/it]\n",
      "epoch[42] time[150.1]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 12.41, reconstructe loss: 0\n",
      "Accuracy on test set: 91.49%, class loss: 14.4, reconstructe loss: 0\n",
      "epoch: 43: 100%|██████████| 53/53 [02:28<00:00,  2.81s/it]\n",
      "epoch[43] time[148.9]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 26.18, reconstructe loss: 0\n",
      "Accuracy on test set: 94.68%, class loss: 13.62, reconstructe loss: 0\n",
      "epoch: 44: 100%|██████████| 53/53 [02:28<00:00,  2.80s/it]\n",
      "epoch[44] time[148.7]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 13.69, reconstructe loss: 0\n",
      "Accuracy on test set: 90.96%, class loss: 16.48, reconstructe loss: 0\n",
      "epoch: 45: 100%|██████████| 53/53 [02:32<00:00,  2.89s/it]\n",
      "epoch[45] time[153.0]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 15.04, reconstructe loss: 0\n",
      "Accuracy on test set: 89.89%, class loss: 18.05, reconstructe loss: 0\n",
      "epoch: 46: 100%|██████████| 53/53 [02:31<00:00,  2.86s/it]\n",
      "epoch[46] time[151.4]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 12.18, reconstructe loss: 0\n",
      "Accuracy on test set: 93.09%, class loss: 14.75, reconstructe loss: 0\n",
      "epoch: 47: 100%|██████████| 53/53 [02:30<00:00,  2.84s/it]\n",
      "epoch[47] time[150.4]s lr:3e-05\n",
      "Accuracy on train set: 99.05%, class loss: 20.96, reconstructe loss: 0\n",
      "Accuracy on test set: 92.02%, class loss: 16.25, reconstructe loss: 0\n",
      "epoch: 48: 100%|██████████| 53/53 [02:30<00:00,  2.84s/it]\n",
      "epoch[48] time[150.6]s lr:3e-05\n",
      "Accuracy on train set: 99.05%, class loss: 15.36, reconstructe loss: 0\n",
      "Accuracy on test set: 93.62%, class loss: 14.68, reconstructe loss: 0\n",
      "epoch: 49: 100%|██████████| 53/53 [02:31<00:00,  2.85s/it]\n",
      "epoch[49] time[151.3]s lr:3e-05\n",
      "Accuracy on train set: 99.17%, class loss: 11.51, reconstructe loss: 0\n",
      "Accuracy on test set: 94.15%, class loss: 13.12, reconstructe loss: 0\n",
      "epoch: 50:   4%|▍         | 2/53 [00:08<03:26,  4.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/3719365090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mweight_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}-oral'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_eval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23/3719365090.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, dataloder)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00007\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                 )\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': net.encoder.parameters(), 'lr': learning_rate},\n",
    "#     {'params': net.decoder.parameters(), 'lr': 0.005}\n",
    "# ])\n",
    "# 动态学习率\n",
    "optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
    "# def lf(epoch):\n",
    "#     # 前20个epoch学习率保持不变，20个epoch后学习率按比例衰减\n",
    "#     if epoch <= 28:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         lr = math.exp(0.2 * (28 - epoch))\n",
    "#         return lr\n",
    "\n",
    "# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "# scheduler.last_epoch = last_epoch - 1\n",
    "# scheduler.step()\n",
    "\n",
    "def train(epoch, dataloder):\n",
    "    net.train()\n",
    "    t0 = time.time()\n",
    "    c = 0.00007\n",
    "    with tqdm(total=len(dataloder), file=sys.stdout) as pbar:\n",
    "        for (X_batch, y_batch) in dataloder:\n",
    "            pbar.set_description('epoch: %d' % epoch)\n",
    "            \n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_hat = net(X_batch, reconstruct=False)\n",
    "    #         loss = margin_loss(y_batch, y_hat) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n",
    "            loss = margin_loss(y_hat, y_batch) # + c * F.mse_loss(X_batch, reconstructed_x, reduction='sum')\n",
    "    #         loss = loss_f(y_hat, y_batch) + c * F.mse_loss(X_batch, reconstructed_x, reduction='mean')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.update(1)\n",
    "    now_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    t1 = time.time()\n",
    "    print(f'epoch[{epoch}] time[{round(t1-t0,1)}]s lr:{now_lr}')\n",
    "#     scheduler.step()\n",
    "    return now_lr\n",
    "\n",
    "\n",
    "def evaluate(data_loader, type):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images, reconstruct=False)\n",
    "            _, predicted = torch.max(outputs.norm(dim=-1), 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss1 += margin_loss(outputs,labels).item()\n",
    "            #loss2 += F.mse_loss(images, reconstructed_x, reduction='sum').item()\n",
    "    eval_loss = loss1 + loss2\n",
    "    acc = 100 * correct / total\n",
    "    print(f'Accuracy on {type} set: {round(acc, 2)}%, class loss: {round(loss1, 2)}, reconstructe loss: {round(loss2, 2)}')\n",
    "    return acc, loss1 / total, loss2 / total, eval_loss / total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    weight_path_best = os.path.join(result_path, 'best')\n",
    "    if not os.path.exists(weight_path_best):\n",
    "        os.mkdir(weight_path_best)\n",
    "    for epoch in range(last_epoch, num_epoch):\n",
    "        weight_name = '{}-oral'.format(model_name)\n",
    "        lr = train(epoch, train_loader)\n",
    "        train_acc, train_loss1, train_loss2, train_eval_loss = evaluate(train_loader, type='train')\n",
    "        test_acc, test_loss1, test_loss2, test_eval_loss = evaluate(test_loader, type='test')\n",
    "        \n",
    "        # 保存训练好的模型之前，删掉已有的模型\n",
    "        for f in glob.glob(result_path + '/**.pth'):\n",
    "            os.remove(f)\n",
    "        weight_name = weight_name + '_epoch{}.pth'.format(epoch + 1)\n",
    "        weight_path = os.path.join(result_path, weight_name)\n",
    "        torch.save(net, weight_path)\n",
    "        \n",
    "        if test_acc > test_acc_best:\n",
    "            test_acc_best = test_acc\n",
    "            for f in glob.glob(weight_path_best + '/**.pth'):\n",
    "                os.remove(f)\n",
    "            torch.save(net, os.path.join(weight_path_best, weight_name))\n",
    "        train_test_data.loc[len(train_test_data.index)] = [train_acc, train_loss1, train_loss2, train_eval_loss,\n",
    "                                                           lr,\n",
    "                                                           test_acc, test_loss1, test_loss2, test_eval_loss,\n",
    "                                                           epoch, test_acc_best]\n",
    "\n",
    "        train_test_data.to_csv(train_test_data_path, index=False)\n",
    "\n",
    "    train_acc_array = train_test_data['train_acc'].values\n",
    "    test_acc_array = train_test_data['test_acc'].values\n",
    "    train_loss_array = train_test_data['train_eval_loss'].values\n",
    "    test_loss_array = train_test_data['test_eval_loss'].values\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_acc_array, label='Training Accuracy')\n",
    "    plt.plot(test_acc_array, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Capsnet Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(train_loss_array, label='Training Loss')\n",
    "    plt.plot(test_loss_array, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Margin Entropy')\n",
    "    plt.title('Capsnet Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T11:27:56.760855Z",
     "iopub.status.busy": "2023-03-22T11:27:56.759864Z",
     "iopub.status.idle": "2023-03-22T11:27:59.581133Z",
     "shell.execute_reply": "2023-03-22T11:27:59.579863Z",
     "shell.execute_reply.started": "2023-03-22T11:27:56.760796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='output.zip' target='_blank'>output.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/output.zip"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!zip -q -r output.zip /kaggle/working/\n",
    "\n",
    "import os\n",
    "os.chdir('/kaggle/working')\n",
    "from IPython.display import FileLink\n",
    "FileLink('output.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
